{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vtm.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorboard\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvtm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collate_topics\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvtm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimelineDataset\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvtm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vtm.dataset'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import tensorboard\n",
    "from torch.utils.data import DataLoader\n",
    "from vtm.dataset import collate_topics\n",
    "from vtm.dataset import TimelineDataset\n",
    "from vtm.model.model import ClassifierModel\n",
    "from vtm.model.model import TimelineModel\n",
    "\n",
    "_FAIL_ON_CPU = flags.DEFINE_boolean('fail_on_cpu', False,\n",
    "                                    'fail if not run on GPU')\n",
    "_DATA_PATH = flags.DEFINE_string('data_path', None, 'The dataset path.')\n",
    "_CHECKPOINT_DIR = flags.DEFINE_string('checkpoint_dir', None,\n",
    "                                      'Directory for saving checkpoints.')\n",
    "_TENSORBOARD_DIR = flags.DEFINE_string(\n",
    "    'tensorboard_dir', None, 'Directory for saving tensorboard events.')\n",
    "_TRAINED_MODEL_PATH = flags.DEFINE_string(\n",
    "    'trained_model_path', None,\n",
    "    'If given, only run inference using the trained model.')\n",
    "\n",
    "# Problem hyperparas\n",
    "_MAX_NUM_CLUSTER = flags.DEFINE_integer('max_num_cluster', 24,\n",
    "                                        'max number of clusters')\n",
    "_MAX_NUM_VIDEO = flags.DEFINE_integer('max_num_video', 120,\n",
    "                                      'max number of videos')\n",
    "_VIDEO_FEATURE = flags.DEFINE_string('video_feature',\n",
    "                                     'vca_video_features_pulsar_embedding',\n",
    "                                     'The used video input feature.')\n",
    "_OFFLINE_DISTILLATION = flags.DEFINE_boolean(\n",
    "    'offline_distillation', False,\n",
    "    ('Apply knowledge distillation if True.'\n",
    "     'The teacher model is the model with text embeddings as input.'))\n",
    "_TRAINED_TEACHER_MODEL_PATH = flags.DEFINE_string(\n",
    "    'trained_teacher_model_path', None,\n",
    "    'The pretrained teacher model path, used for distillation.')\n",
    "_ONLINE_DISTILLATION = flags.DEFINE_boolean(\n",
    "    'online_distillation', False,\n",
    "    ('Apply online knowledge distillation if True.'\n",
    "     'The teacher model is the model with text embeddings as input.'))\n",
    "_FEATURE_DISTILLATION = flags.DEFINE_boolean(\n",
    "    'feature_distillation', False,\n",
    "    ('Distill the intermediate features if True.'\n",
    "     'The teacher model is the model with text embeddings as input.'))\n",
    "\n",
    "# Model hyperparas\n",
    "_RUN_BASELINE = flags.DEFINE_boolean(\n",
    "    'run_baseline', False,\n",
    "    'Run the baseline model if True; otherwise run our model.')\n",
    "_REMOVE_VIDEO_AND_CLUSTER_ENCODERS = flags.DEFINE_boolean(\n",
    "    'remove_video_and_cluster_encoders', False,\n",
    "    'Remove video and cluster corresponding encoders if True.')\n",
    "_SEMANTICS_AWARE_HEAD = flags.DEFINE_boolean(\n",
    "    'semantics_aware_head', False, 'Add the semantics-aware head if True.')\n",
    "_CONTRASTIVE_LOSS = flags.DEFINE_boolean(\n",
    "    'contrastive_loss', False,\n",
    "    ('Use contrastive loss for the semantics-aware head if True.'\n",
    "     'Otherwise, use the consine similarity loss.'))\n",
    "_TEMPERATURE = flags.DEFINE_float(\n",
    "    'temperature', 0.07, 'Temperature value used for contrastive loss.')\n",
    "_SEMANTICS_AWARE_HEAD_POS = flags.DEFINE_enum(\n",
    "    'semantics_aware_head_pos', 'pos1', ['pos1', 'pos2'],\n",
    "    'The position to place the semantics-aware head.')\n",
    "_TEXT_EMBEDDING_AS_INPUT = flags.DEFINE_boolean(\n",
    "    'text_embedding_as_input', False,\n",
    "    'Include the text embeddings as input if True.')\n",
    "_NUM_EMB = flags.DEFINE_integer(\n",
    "    'num_emb', 256,\n",
    "    'number of hidden dimensions for learnable cluster embeddings')\n",
    "_NUM_INPUT_HIDDEN_VIDEO = flags.DEFINE_integer(\n",
    "    'num_input_hidden_video', 256,\n",
    "    'number of hidden dimensions for input video embeddings')\n",
    "_NUM_HIDDEN = flags.DEFINE_integer(\n",
    "    'num_hidden', 256, 'number of hidden dimensions in Transformer encoders')\n",
    "_NUM_HEAD = flags.DEFINE_integer(\n",
    "    'num_head', 2, 'number of attention heads in Transformer encoders')\n",
    "_NUM_LAYERS = flags.DEFINE_integer('num_layers', 1,\n",
    "                                   'number of layers in Transformer encoders')\n",
    "_VIDEO_PE = flags.DEFINE_boolean('video_pe', False,\n",
    "                                 'if apply positional encoding to videos')\n",
    "_DROPOUT = flags.DEFINE_float('dropout', 0.1,\n",
    "                              'dropout rate in Transformer encoders')\n",
    "_SEMANTICS_LOSS_WEIGHT = flags.DEFINE_float(\n",
    "    'semantics_loss_weight', 1, 'the weight for the semantics-aware head loss')\n",
    "_DISTILLATION_LOSS_WEIGHT = flags.DEFINE_float(\n",
    "    'distillation_loss_weight', 0.1, 'the weight for the distillation loss')\n",
    "_TEACHER_LOSS_WEIGHT = flags.DEFINE_float(\n",
    "    'teacher_loss_weight', 1,\n",
    "    'the weight for the teacher model loss during online distillation')\n",
    "\n",
    "# Training hyperparas\n",
    "_BATCH_SIZE = flags.DEFINE_integer('batch_size', 16, 'batch size')\n",
    "_EPOCHS = flags.DEFINE_integer('epochs', 10, 'epochs')\n",
    "_LOG_STEPSIZE = flags.DEFINE_integer('log_stepsize', 100, 'log step size')\n",
    "_LEARNING_RATE = flags.DEFINE_float('learning_rate', 0.01, 'learning rate')\n",
    "_WEIGHT_DECAY = flags.DEFINE_float('weight_decay', 0.00001, 'weight decay')\n",
    "\n",
    "\n",
    "def check_gpu():\n",
    "  \"\"\"Print GPU info and return 'cuda' if found, 'cpu' otherwise.\"\"\"\n",
    "  try:\n",
    "    logging.info('FLAGS.fail_on_cpu: %s', _FAIL_ON_CPU.value)\n",
    "    logging.info('torch.__version__: %s', torch.__version__)\n",
    "    logging.info('torch.cuda.device_count(): %s', torch.cuda.device_count())\n",
    "    logging.info('torch.cuda.current_device(): %s', torch.cuda.current_device())\n",
    "    logging.info('torch.cuda.get_device_name(0): %s',\n",
    "                 torch.cuda.get_device_name(0))\n",
    "    logging.info('torch.cuda.is_available(0): %s', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "      return 'cuda'\n",
    "  except Exception as e:  # pylint: disable=broad-except\n",
    "    logging.warning(e)\n",
    "  if _FAIL_ON_CPU.value:\n",
    "    logging.error('Not able to run on CPU')\n",
    "    exit(1)\n",
    "  logging.error('Falling back to CPU.')\n",
    "  return 'cpu'\n",
    "\n",
    "\n",
    "# We cannot use built-in types (tuple/list/dict) for parametric annotations (\n",
    "# supported by python >=3.9), since the xcloud pre-built PyTorch image only\n",
    "# supports up to python 3.8.\n",
    "def get_dataset():\n",
    "  \"\"\"Initialize datasets.\"\"\"\n",
    "  train_dataset = TimelineDataset(\n",
    "      partition='train',\n",
    "      feature_key=_VIDEO_FEATURE.value,\n",
    "      feature_dim=_NUM_INPUT_HIDDEN_VIDEO.value,\n",
    "      data_path=_DATA_PATH.value)\n",
    "  valid_dataset = TimelineDataset(\n",
    "      partition='valid',\n",
    "      feature_key=_VIDEO_FEATURE.value,\n",
    "      feature_dim=_NUM_INPUT_HIDDEN_VIDEO.value,\n",
    "      data_path=_DATA_PATH.value)\n",
    "  test_dataset = TimelineDataset(\n",
    "      partition='test',\n",
    "      feature_key=_VIDEO_FEATURE.value,\n",
    "      feature_dim=_NUM_INPUT_HIDDEN_VIDEO.value,\n",
    "      data_path=_DATA_PATH.value)\n",
    "  return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def train_epoch(timeline_model,\n",
    "                device,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                teacher_model = None):\n",
    "  \"\"\"Training loop for one epoch.\"\"\"\n",
    "  timeline_model.train()\n",
    "  train_loss = 0\n",
    "  total_video = 0\n",
    "  if _SEMANTICS_AWARE_HEAD.value:\n",
    "    train_semantics_loss = 0\n",
    "    total_clusters = 0\n",
    "  if _OFFLINE_DISTILLATION.value:\n",
    "    teacher_model.eval()\n",
    "    train_distillation_loss = 0\n",
    "    total_clusters = 0\n",
    "  if _ONLINE_DISTILLATION.value:\n",
    "    teacher_model.train()\n",
    "    train_distillation_loss = 0\n",
    "    train_teacher_model_loss = 0\n",
    "    total_clusters = 0\n",
    "  for step, data_batch in enumerate(train_loader):\n",
    "    for key in data_batch:\n",
    "      data_batch[key] = data_batch[key].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    if _SEMANTICS_AWARE_HEAD.value:\n",
    "      log_score, cluster_semantics_h, _ = timeline_model(data_batch)\n",
    "    else:\n",
    "      log_score, cluster_intermediate_h, video_intermediate_h = timeline_model(\n",
    "          data_batch)\n",
    "    # Only compute loss for non-padding video tokens\n",
    "    log_score = log_score.view(-1, _MAX_NUM_CLUSTER.value)\n",
    "    video_cluster_label = data_batch['video_cluster_label'].view(-1)\n",
    "    video_non_padding_mask = ~data_batch['video_padding_mask'].view(-1)\n",
    "    loss = F.nll_loss(\n",
    "        log_score[video_non_padding_mask],\n",
    "        video_cluster_label[video_non_padding_mask],\n",
    "        reduction='sum')\n",
    "\n",
    "    # With semantics-aware head: compute loss for non-padding cluster tokens\n",
    "    if _SEMANTICS_AWARE_HEAD.value:\n",
    "      if _CONTRASTIVE_LOSS.value:\n",
    "        # (max_num_cluster, num_emb, batch_size)\n",
    "        cluster_semantics_h = cluster_semantics_h.permute((1, 2, 0))\n",
    "        # (max_num_cluster, max_num_cluster, batch_size)\n",
    "        cosine_similarity_pairwise = F.cosine_similarity(\n",
    "            cluster_semantics_h, cluster_semantics_h.unsqueeze(1), dim=-2)\n",
    "        # (batch_size, max_num_cluster, max_num_cluster)\n",
    "        cosine_similarity_pairwise = cosine_similarity_pairwise.permute(\n",
    "            (2, 0, 1)) / _TEMPERATURE.value\n",
    "        # (batch_size, max_num_cluster)\n",
    "        self_cosine_similarity = torch.diagonal(\n",
    "            cosine_similarity_pairwise, dim1=-2, dim2=-1)\n",
    "        semantics_loss = (\n",
    "            -self_cosine_similarity[data_batch['cluster_non_padding_mask']] +\n",
    "            torch.logsumexp(\n",
    "                cosine_similarity_pairwise[\n",
    "                    data_batch['cluster_non_padding_mask']],\n",
    "                dim=-1)).sum()\n",
    "      else:\n",
    "        semantics_loss = (1 - F.cosine_similarity(\n",
    "            cluster_semantics_h[data_batch['cluster_non_padding_mask']],\n",
    "            data_batch['cluster_text_features'][\n",
    "                data_batch['cluster_non_padding_mask']])).sum()\n",
    "      loss_sum = loss + _SEMANTICS_LOSS_WEIGHT.value * semantics_loss\n",
    "      loss_sum.backward()\n",
    "      optimizer.step()\n",
    "      if step % _LOG_STEPSIZE.value == 0:\n",
    "        logging.info('[%s/%s] Loss: %s',\n",
    "                     step * len(data_batch['video_features']),\n",
    "                     len(train_loader.dataset),\n",
    "                     loss.item() / video_non_padding_mask.sum().item())\n",
    "        logging.info(\n",
    "            '[%s/%s] Semantics Loss: %s',\n",
    "            step * len(data_batch['video_features']), len(train_loader.dataset),\n",
    "            semantics_loss.item() /\n",
    "            data_batch['cluster_non_padding_mask'].sum().item())\n",
    "      train_semantics_loss += semantics_loss.item()\n",
    "      train_loss += loss.item()\n",
    "      total_video += video_non_padding_mask.sum().item()\n",
    "      total_clusters += data_batch['cluster_non_padding_mask'].sum().item()\n",
    "\n",
    "    # Offline knowledge distillation\n",
    "    elif _OFFLINE_DISTILLATION.value:\n",
    "      with torch.no_grad():\n",
    "        teacher_log_score, teacher_cluster_h, teacher_video_h = teacher_model(\n",
    "            data_batch)\n",
    "      teacher_log_score = teacher_log_score.view(-1, _MAX_NUM_CLUSTER.value)\n",
    "      if _FEATURE_DISTILLATION.value:\n",
    "        distillation_cluster_loss = torch.norm(\n",
    "            teacher_cluster_h - cluster_intermediate_h, dim=-1).sum()\n",
    "        distillation_video_loss = torch.norm(\n",
    "            teacher_video_h[~data_batch['video_padding_mask']] -\n",
    "            video_intermediate_h[~data_batch['video_padding_mask']],\n",
    "            dim=-1).sum()\n",
    "        distillation_loss = distillation_cluster_loss + distillation_video_loss\n",
    "      else:\n",
    "        distillation_loss = F.kl_div(\n",
    "            log_score[video_non_padding_mask],\n",
    "            teacher_log_score[video_non_padding_mask],\n",
    "            reduction='sum',\n",
    "            log_target=True)\n",
    "      loss_sum = loss + _DISTILLATION_LOSS_WEIGHT.value * distillation_loss\n",
    "      loss_sum.backward()\n",
    "      optimizer.step()\n",
    "      if step % _LOG_STEPSIZE.value == 0:\n",
    "        logging.info('[%s/%s] Loss: %s',\n",
    "                     step * len(data_batch['video_features']),\n",
    "                     len(train_loader.dataset),\n",
    "                     loss.item() / video_non_padding_mask.sum().item())\n",
    "        logging.info(\n",
    "            '[%s/%s] Distillation Loss: %s',\n",
    "            step * len(data_batch['video_features']), len(train_loader.dataset),\n",
    "            distillation_loss.item() /\n",
    "            (video_non_padding_mask.sum().item() +\n",
    "             torch.numel(data_batch['cluster_non_padding_mask'])))\n",
    "      train_distillation_loss += distillation_loss.item()\n",
    "      train_loss += loss.item()\n",
    "      total_video += video_non_padding_mask.sum().item()\n",
    "      total_clusters += data_batch['cluster_non_padding_mask'].sum().item()\n",
    "\n",
    "    # Online knowledge distillation\n",
    "    elif _ONLINE_DISTILLATION.value:\n",
    "      teacher_log_score, teacher_cluster_h, teacher_video_h = teacher_model(\n",
    "          data_batch)\n",
    "      teacher_log_score = teacher_log_score.view(-1, _MAX_NUM_CLUSTER.value)\n",
    "      if _FEATURE_DISTILLATION.value:\n",
    "        distillation_cluster_loss = torch.norm(\n",
    "            teacher_cluster_h - cluster_intermediate_h, dim=-1).sum()\n",
    "        distillation_video_loss = torch.norm(\n",
    "            teacher_video_h[~data_batch['video_padding_mask']] -\n",
    "            video_intermediate_h[~data_batch['video_padding_mask']],\n",
    "            dim=-1).sum()\n",
    "        distillation_loss = distillation_cluster_loss + distillation_video_loss\n",
    "      else:\n",
    "        distillation_loss = F.kl_div(\n",
    "            log_score[video_non_padding_mask],\n",
    "            teacher_log_score[video_non_padding_mask],\n",
    "            reduction='sum',\n",
    "            log_target=True)\n",
    "      teacher_model_loss = F.nll_loss(\n",
    "          teacher_log_score[video_non_padding_mask],\n",
    "          video_cluster_label[video_non_padding_mask],\n",
    "          reduction='sum')\n",
    "      loss_sum = (loss + _DISTILLATION_LOSS_WEIGHT.value * distillation_loss\n",
    "                  + _TEACHER_LOSS_WEIGHT.value * teacher_model_loss)\n",
    "      loss_sum.backward()\n",
    "      optimizer.step()\n",
    "      if step % _LOG_STEPSIZE.value == 0:\n",
    "        logging.info('[%s/%s] Loss: %s',\n",
    "                     step * len(data_batch['video_features']),\n",
    "                     len(train_loader.dataset),\n",
    "                     loss.item() / video_non_padding_mask.sum().item())\n",
    "        logging.info(\n",
    "            '[%s/%s] Teacher Model Loss: %s',\n",
    "            step * len(data_batch['video_features']), len(train_loader.dataset),\n",
    "            teacher_model_loss.item() / video_non_padding_mask.sum().item())\n",
    "        logging.info(\n",
    "            '[%s/%s] Distillation Loss: %s',\n",
    "            step * len(data_batch['video_features']), len(train_loader.dataset),\n",
    "            distillation_loss.item() /\n",
    "            (video_non_padding_mask.sum().item() +\n",
    "             torch.numel(data_batch['cluster_non_padding_mask'])))\n",
    "      train_distillation_loss += distillation_loss.item()\n",
    "      train_loss += loss.item()\n",
    "      train_teacher_model_loss += teacher_model_loss.item()\n",
    "      total_video += video_non_padding_mask.sum().item()\n",
    "      total_clusters += data_batch['cluster_non_padding_mask'].sum().item()\n",
    "\n",
    "    else:\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if step % _LOG_STEPSIZE.value == 0:\n",
    "        logging.info('[%s/%s] Loss: %s',\n",
    "                     step * len(data_batch['video_features']),\n",
    "                     len(train_loader.dataset),\n",
    "                     loss.item() / video_non_padding_mask.sum().item())\n",
    "      train_loss += loss.item()\n",
    "      total_video += video_non_padding_mask.sum().item()\n",
    "\n",
    "  if _SEMANTICS_AWARE_HEAD.value:\n",
    "    return train_loss / total_video, train_semantics_loss / total_clusters, None\n",
    "  elif _OFFLINE_DISTILLATION.value:\n",
    "    return train_loss / total_video, train_distillation_loss / (\n",
    "        total_video + total_clusters), None\n",
    "  elif _ONLINE_DISTILLATION.value:\n",
    "    return train_loss / total_video, train_distillation_loss / (\n",
    "        total_video + total_clusters), train_teacher_model_loss / total_video,\n",
    "  else:\n",
    "    return train_loss / total_video, None, None\n",
    "\n",
    "\n",
    "def evaluate(timeline_model, device,\n",
    "             loader):\n",
    "  \"\"\"Evaluation pipeline for measuring video to cluster accuracy (float).\"\"\"\n",
    "  timeline_model.eval()\n",
    "  video_to_cluster_correct = 0\n",
    "  total_video = 0\n",
    "  with torch.no_grad():\n",
    "    for data_batch in loader:\n",
    "      for key in [\n",
    "          'video_features', 'video_padding_mask', 'video_cluster_label'\n",
    "      ]:\n",
    "        data_batch[key] = data_batch[key].to(device)\n",
    "      log_score, _, _ = timeline_model(data_batch)\n",
    "      log_score = log_score.view(-1, _MAX_NUM_CLUSTER.value)\n",
    "      prediction = log_score.argmax(dim=1, keepdim=False)\n",
    "      video_cluster_label = data_batch['video_cluster_label'].view(-1)\n",
    "      video_non_padding_mask = ~data_batch['video_padding_mask'].view(-1)\n",
    "      video_to_cluster_correct += prediction[video_non_padding_mask].eq(\n",
    "          video_cluster_label[video_non_padding_mask]).sum().item()\n",
    "      total_video += video_non_padding_mask.sum().item()\n",
    "  video_to_cluster_accuracy = video_to_cluster_correct / total_video\n",
    "  return video_to_cluster_accuracy\n",
    "\n",
    "\n",
    "def inference(timeline_model, device,\n",
    "              dataset):\n",
    "  \"\"\"Inference with one-by-one processing.\"\"\"\n",
    "  timeline_model.eval()\n",
    "  video_to_cluster_correct = 0\n",
    "  total_video = 0\n",
    "  final_predictions = []\n",
    "  loader = DataLoader(\n",
    "      dataset, batch_size=1, shuffle=False, collate_fn=collate_topics)\n",
    "  with torch.no_grad():\n",
    "    for i, data_batch in enumerate(loader):\n",
    "      data_prediction = {}\n",
    "      for key in [\n",
    "          'video_features', 'video_padding_mask', 'video_cluster_label'\n",
    "      ]:\n",
    "        data_batch[key] = data_batch[key].to(device)\n",
    "      log_score, _, _ = timeline_model(data_batch)\n",
    "      log_score = log_score.view(-1, _MAX_NUM_CLUSTER.value)\n",
    "      prediction = log_score.argmax(dim=1, keepdim=False)\n",
    "      video_cluster_label = data_batch['video_cluster_label'].view(-1)\n",
    "      video_non_padding_mask = ~data_batch['video_padding_mask'].view(-1)\n",
    "      video_to_cluster_correct += prediction[video_non_padding_mask].eq(\n",
    "          video_cluster_label[video_non_padding_mask]).sum().item()\n",
    "      total_video += video_non_padding_mask.sum().item()\n",
    "      data_prediction['timeline_url'] = dataset[i]['timeline_url']\n",
    "      data_prediction['pred'] = prediction.tolist()\n",
    "      data_prediction['label'] = video_cluster_label.tolist()\n",
    "      final_predictions.append(data_prediction)\n",
    "\n",
    "  video_to_cluster_accuracy = video_to_cluster_correct / total_video\n",
    "  return final_predictions, video_to_cluster_accuracy\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, output_dir):\n",
    "  \"\"\"Save model to GCS.\"\"\"\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  # Will overwrite existing previously saved model.\n",
    "  torch.save(model.state_dict(), os.path.join(output_dir, 'model.pt'))\n",
    "  torch.save(\n",
    "      {\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "      }, os.path.join(output_dir, 'checkpoint.tar'))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  logging.info('Job started')\n",
    "  device = torch.device(check_gpu())\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  logging.info('FLAGS.epochs: %s', _EPOCHS.value)\n",
    "  logging.info('FLAGS.batch_size: %s', _BATCH_SIZE.value)\n",
    "  logging.info('FLAGS.learning_rate: %s', _LEARNING_RATE.value)\n",
    "  logging.info('FLAGS.weight_decay: %s', _WEIGHT_DECAY.value)\n",
    "\n",
    "  if _RUN_BASELINE.value:\n",
    "    logging.info('Running baseline model.')\n",
    "    model = ClassifierModel(_MAX_NUM_CLUSTER.value, _MAX_NUM_VIDEO.value,\n",
    "                            _NUM_EMB.value, _NUM_INPUT_HIDDEN_VIDEO.value,\n",
    "                            _NUM_HIDDEN.value, _NUM_HEAD.value,\n",
    "                            _NUM_LAYERS.value, _VIDEO_PE.value, _DROPOUT.value)\n",
    "  else:\n",
    "    if _SEMANTICS_AWARE_HEAD.value:\n",
    "      logging.info('Running our complete model.')\n",
    "    else:\n",
    "      if _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value:\n",
    "        logging.info('Running our model without the semantics-aware head.')\n",
    "        logging.info('and the video and cluster encoders.')\n",
    "      else:\n",
    "        logging.info('Running our model without the semantics-aware head.')\n",
    "    model = TimelineModel(_MAX_NUM_CLUSTER.value, _MAX_NUM_VIDEO.value,\n",
    "                          _NUM_EMB.value, _NUM_INPUT_HIDDEN_VIDEO.value,\n",
    "                          _NUM_HIDDEN.value, _NUM_HEAD.value, _NUM_LAYERS.value,\n",
    "                          _VIDEO_PE.value, _DROPOUT.value,\n",
    "                          _SEMANTICS_AWARE_HEAD.value,\n",
    "                          _SEMANTICS_AWARE_HEAD_POS.value,\n",
    "                          _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value,\n",
    "                          _TEXT_EMBEDDING_AS_INPUT.value)\n",
    "    if _OFFLINE_DISTILLATION.value or _ONLINE_DISTILLATION.value:\n",
    "      assert not _SEMANTICS_AWARE_HEAD.value\n",
    "      assert not _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value\n",
    "      teacher_model = TimelineModel(\n",
    "          _MAX_NUM_CLUSTER.value, _MAX_NUM_VIDEO.value, _NUM_EMB.value,\n",
    "          _NUM_INPUT_HIDDEN_VIDEO.value, _NUM_HIDDEN.value, _NUM_HEAD.value,\n",
    "          _NUM_LAYERS.value, _VIDEO_PE.value, _DROPOUT.value,\n",
    "          _SEMANTICS_AWARE_HEAD.value, _SEMANTICS_AWARE_HEAD_POS.value,\n",
    "          _REMOVE_VIDEO_AND_CLUSTER_ENCODERS.value, True)\n",
    "      teacher_model = nn.DataParallel(teacher_model).to(device)\n",
    "      if _OFFLINE_DISTILLATION.value:\n",
    "        logging.info('Performing offline knowledge distillation.')\n",
    "        teacher_model.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(_TRAINED_TEACHER_MODEL_PATH.value, 'model.pt')))\n",
    "        logging.info('Loaded pretrained teacher model.')\n",
    "      elif _ONLINE_DISTILLATION.value:\n",
    "        logging.info('Performing online knowledge distillation.')\n",
    "  train_dataset, valid_dataset, test_dataset = get_dataset()\n",
    "  trained_model_path = _TRAINED_MODEL_PATH.value\n",
    "  if trained_model_path is not None:\n",
    "    logging.info('Run inference only.')\n",
    "    timeline_model = nn.DataParallel(model).to(device)\n",
    "    timeline_model.load_state_dict(\n",
    "        torch.load(os.path.join(trained_model_path, 'model.pt')))\n",
    "    valid_predictions, final_valid_v2c_acc = inference(timeline_model, device,\n",
    "                                                       valid_dataset)\n",
    "    test_predictions, final_test_v2c_acc = inference(timeline_model, device,\n",
    "                                                     test_dataset)\n",
    "    logging.info('Final Valid Acc %s', final_valid_v2c_acc)\n",
    "    logging.info('Final Test Acc %s', final_test_v2c_acc)\n",
    "    with open(os.path.join(trained_model_path, 'valid_prediction.json'),\n",
    "              'w') as f:\n",
    "      json.dump(valid_predictions, f)\n",
    "    with open(os.path.join(trained_model_path, 'test_prediction.json'),\n",
    "              'w') as f:\n",
    "      json.dump(test_predictions, f)\n",
    "  else:\n",
    "    timeline_model = nn.DataParallel(model).to(device)\n",
    "\n",
    "    if _OFFLINE_DISTILLATION.value or _ONLINE_DISTILLATION.value:\n",
    "      optimizer = optim.Adam(\n",
    "          list(timeline_model.parameters()) + list(teacher_model.parameters()),\n",
    "          lr=_LEARNING_RATE.value,\n",
    "          weight_decay=_WEIGHT_DECAY.value)\n",
    "    else:\n",
    "      optimizer = optim.Adam(\n",
    "          timeline_model.parameters(),\n",
    "          lr=_LEARNING_RATE.value,\n",
    "          weight_decay=_WEIGHT_DECAY.value)\n",
    "\n",
    "    if _TENSORBOARD_DIR.value:\n",
    "      writer = tensorboard.SummaryWriter(_TENSORBOARD_DIR.value)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=_BATCH_SIZE.value,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_topics)\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=_BATCH_SIZE.value,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_topics)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=_BATCH_SIZE.value,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_topics)\n",
    "\n",
    "    best_valid_v2c_acc = 0\n",
    "    for epoch in range(1, _EPOCHS.value + 1):\n",
    "      logging.info('Epoch %s of %s', epoch, _EPOCHS.value)\n",
    "      if _SEMANTICS_AWARE_HEAD.value:\n",
    "        train_loss, semantics_loss, _ = train_epoch(timeline_model, device,\n",
    "                                                    train_loader, optimizer)\n",
    "        logging.info('Loss %s', train_loss)\n",
    "        logging.info('Semantics loss %s', semantics_loss)\n",
    "      elif _OFFLINE_DISTILLATION.value:\n",
    "        train_loss, distillation_loss, _ = train_epoch(timeline_model, device,\n",
    "                                                       train_loader, optimizer,\n",
    "                                                       teacher_model)\n",
    "        logging.info('Loss %s', train_loss)\n",
    "        logging.info('Distillation loss %s', distillation_loss)\n",
    "      elif _ONLINE_DISTILLATION.value:\n",
    "        train_loss, distillation_loss, train_teacher_model_loss = train_epoch(\n",
    "            timeline_model, device, train_loader, optimizer, teacher_model)\n",
    "        logging.info('Loss %s', train_loss)\n",
    "        logging.info('Teacher Model Loss %s', train_teacher_model_loss)\n",
    "        logging.info('Distillation loss %s', distillation_loss)\n",
    "      else:\n",
    "        train_loss, _, _ = train_epoch(timeline_model, device, train_loader,\n",
    "                                       optimizer)\n",
    "        logging.info('Loss %s', train_loss)\n",
    "      train_v2c_acc = evaluate(timeline_model, device, train_loader)\n",
    "      valid_v2c_acc = evaluate(timeline_model, device, valid_loader)\n",
    "      test_v2c_acc = evaluate(timeline_model, device, test_loader)\n",
    "      if valid_v2c_acc > best_valid_v2c_acc:\n",
    "        best_valid_v2c_acc = valid_v2c_acc\n",
    "        final_test_v2c_acc = test_v2c_acc\n",
    "        if _CHECKPOINT_DIR.value:\n",
    "          save_model(timeline_model, optimizer, _CHECKPOINT_DIR.value)\n",
    "      logging.info('Training Acc %s', train_v2c_acc)\n",
    "      logging.info('Valid Acc %s', valid_v2c_acc)\n",
    "      logging.info('Test Acc %s', test_v2c_acc)\n",
    "      logging.info('Best Valid Acc So Far %s', best_valid_v2c_acc)\n",
    "      logging.info('Final Test Acc So Far %s', final_test_v2c_acc)\n",
    "\n",
    "      if _TENSORBOARD_DIR.value:\n",
    "        if _SEMANTICS_AWARE_HEAD.value:\n",
    "          writer.add_scalar('Semantics Loss/train', semantics_loss, epoch)\n",
    "        if _OFFLINE_DISTILLATION.value:\n",
    "          writer.add_scalar('Distillation Loss/train', distillation_loss, epoch)\n",
    "        if _ONLINE_DISTILLATION.value:\n",
    "          writer.add_scalar('Distillation Loss/train', distillation_loss, epoch)\n",
    "          writer.add_scalar('Teacher Model Loss/train',\n",
    "                            train_teacher_model_loss, epoch)\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_v2c_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/valid', valid_v2c_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_v2c_acc, epoch)\n",
    "        logging.info('Flushing TensorBoard writer')\n",
    "        writer.flush()\n",
    "\n",
    "    if _TENSORBOARD_DIR.value:\n",
    "      writer.close()\n",
    "\n",
    "  logging.info('Job finished')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collate_topics, TimelineDataset\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .dataset import collate_topics, TimelineDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path_to_your_project_directory')  # Replace with the path to your project directory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
